<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Explainable AI - Skolyn Documentation</title>
<link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700&family=Google+Sans+Text:wght@400;500&display=swap" rel="stylesheet">
<style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:'Google Sans Text','Google Sans',Roboto,'Helvetica Neue',Arial,sans-serif;max-width:900px;margin:0 auto;padding:48px 40px;color:#1d1d1f;line-height:1.8;font-size:15px}.doc-id-bar{display:flex;justify-content:space-between;align-items:center;padding:12px 20px;background:#f8f9fa;border:1px solid #e8eaed;border-radius:8px;margin-bottom:24px;font-size:13px;color:#5f6368}.doc-id-bar .doc-id{font-family:'Google Sans',sans-serif;font-weight:700;color:#1a73e8;font-size:14px;letter-spacing:.5px}.doc-id-bar .doc-class{padding:4px 12px;background:#e8f0fe;color:#1a73e8;border-radius:100px;font-weight:500;font-size:12px}.doc-header{border-bottom:3px solid #00897b;padding-bottom:28px;margin-bottom:40px}.doc-header h1{font-family:'Google Sans',sans-serif;font-size:32px;font-weight:700;margin-bottom:8px}.doc-header .subtitle{font-size:16px;color:#5f6368;margin-bottom:16px}.doc-header .meta-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:12px;font-size:13px;color:#5f6368}.doc-header .meta-item{display:flex;align-items:center;gap:6px}.doc-header .meta-item strong{color:#1d1d1f}h2{font-family:'Google Sans',sans-serif;font-size:22px;color:#00897b;margin:40px 0 16px;padding-bottom:8px;border-bottom:1px solid #e8eaed}h3{font-family:'Google Sans',sans-serif;font-size:17px;color:#202124;margin:28px 0 12px}p{margin-bottom:14px}ul,ol{margin:0 0 18px 24px}li{margin-bottom:8px}table{width:100%;border-collapse:collapse;margin:16px 0 24px;font-size:13px}th,td{padding:10px 14px;text-align:left;border:1px solid #dadce0}th{background:#e0f2f1;font-weight:600;color:#00695c}td{color:#3c4043}tr:nth-child(even) td{background:#fafafa}code{background:#f1f3f4;padding:2px 8px;border-radius:4px;font-size:13px}.signature-section{margin-top:64px;page-break-inside:avoid}.signature-section .section-title{font-family:'Google Sans',sans-serif;font-size:18px;font-weight:700;color:#1d1d1f;margin-bottom:24px;padding-bottom:12px;border-bottom:2px solid #1d1d1f}.signature-grid{display:grid;grid-template-columns:1fr 1fr;gap:48px}.signature-block{border:1px solid #dadce0;border-radius:12px;padding:32px;background:#fafafa}.signature-block .role{font-family:'Google Sans',sans-serif;font-size:16px;font-weight:700;margin-bottom:4px}.signature-block .name{font-size:14px;color:#5f6368;margin-bottom:24px}.signature-line{border-bottom:1px solid #1d1d1f;height:48px;margin-bottom:8px}.signature-label{font-size:12px;color:#80868b;margin-bottom:16px}.stamp-area{border:2px dashed #dadce0;border-radius:8px;height:100px;display:flex;align-items:center;justify-content:center;color:#80868b;font-size:12px;margin-top:16px}.doc-footer{margin-top:48px;padding-top:24px;border-top:2px solid #e8eaed;font-size:12px;color:#5f6368;text-align:center}.doc-footer p{margin-bottom:4px}@media print{body{max-width:100%;padding:20px;font-size:12px}.signature-section{page-break-before:always}}</style>
</head>
<body>

<div class="doc-id-bar"><span class="doc-id">SKL-AI-001</span><span>Skolyn Platform Documentation</span><span class="doc-class">CONFIDENTIAL</span></div>

<div class="doc-header">
  <h1>Explainable AI</h1>
  <div class="subtitle">Interpretability Framework: Attention Heatmaps, SHAP Values, Natural-Language Rationales, and Clinician Trust Calibration</div>
  <div class="meta-grid">
    <div class="meta-item"><strong>Version:</strong> 3.0</div>
    <div class="meta-item"><strong>Last Updated:</strong> February 2026</div>
    <div class="meta-item"><strong>Classification:</strong> Confidential</div>
    <div class="meta-item"><strong>Owner:</strong> AI Research</div>
  </div>
</div>

<h2>1. Overview</h2>
<p>Explainability is essential for clinical adoption of AI in medical imaging. Radiologists cannot act on AI predictions they do not understand, and regulatory bodies (FDA, EU MDR, MHRA) increasingly require transparency in algorithmic decision-making for medical devices. Skolyn's Explainable AI (XAI) framework provides multi-layered interpretability: visual explanations (attention heatmaps showing which image regions drove the AI's decision), quantitative feature attribution (SHAP values and gradient-based attributions for structured features), natural-language rationales (automatically generated text summarizing the AI's reasoning in clinical terminology), and confidence calibration (ensuring that the AI's stated confidence accurately reflects the probability of correctness). The XAI framework operates across all Terbium OS analysis modules — MRI, CT, Ultrasound, and X-Ray — providing consistent, modality-aware explanations that integrate directly into the radiologist's reading workflow.</p>

<h2>2. Visual Explainability</h2>
<h3>2.1 Grad-CAM++ Attention Maps</h3>
<p>For every pathology detection, the system generates Gradient-weighted Class Activation Maps (Grad-CAM++) that highlight the image regions contributing most to the positive classification. The attention maps are rendered as semi-transparent color overlays on the original image using a jet colormap (red = high attention, blue = low attention), with adjustable opacity controlled by the radiologist. Attention maps serve a dual purpose: they help the radiologist verify that the AI is attending to the correct anatomical location (rather than artifactual features), and they provide spatial localization for findings that do not have discrete bounding boxes.</p>

<h3>2.2 Counterfactual Explanations</h3>
<p>For selected high-complexity findings, the XAI framework generates counterfactual explanations: "What would need to change in the image for the AI to change its prediction?" These explanations highlight the minimal set of image features that, if removed or altered, would flip the classification from positive to negative. Counterfactual explanations are particularly valuable for borderline findings where the clinician needs to understand which specific features are tipping the balance toward a positive classification.</p>

<h2>3. Feature Attribution</h2>
<p>For structured data inputs (patient demographics, clinical history, lab values) that feed into multi-modal ensemble models, SHAP (SHapley Additive exPlanations) values quantify each feature's marginal contribution to the prediction. SHAP values are presented as a waterfall chart showing the base prediction rate, each feature's positive or negative contribution, and the final prediction. This allows clinicians to understand, for example, that a lung nodule risk score was elevated primarily because of the patient's smoking history and nodule growth rate, with moderate contribution from nodule morphology and minimal contribution from patient age. SHAP explanations bridge the gap between the AI's internal representation and the clinical reasoning that radiologists understand.</p>

<h2>4. Natural-Language Rationales</h2>
<p>The natural-language generation (NLG) module converts the AI's structured findings and attention patterns into human-readable clinical text. Rather than simply stating "Pneumothorax detected, confidence 94%," the system generates: "A left-sided pneumothorax is identified with approximately 30% lung volume loss. The pleural line is visible as a curvilinear opacity at the lateral mid-lung zone, with absence of lung markings peripheral to this line. No tension physiology is identified — the mediastinum is midline and the hemidiaphragms are normally positioned." The NLG module uses a medical language model fine-tuned on 50,000 paired image-report examples, ensuring that generated text follows standard radiological reporting conventions and uses appropriate ACR-approved terminology.</p>

<h2>5. Confidence Calibration</h2>
<p>Model confidence calibration ensures that when the AI reports 90% confidence for a finding, the finding is truly positive approximately 90% of the time. Skolyn uses temperature scaling and isotonic regression for post-hoc calibration, trained on a held-out calibration dataset representative of the deployment population. Calibration is monitored continuously using reliability diagrams and Expected Calibration Error (ECE), with recalibration triggered when ECE exceeds 0.05. Well-calibrated confidence scores are essential for clinical workflow integration — they determine whether a finding triggers an immediate alert (high confidence critical finding) or a soft notification (moderate confidence, requires radiologist review).</p>

<h2>6. Regulatory Compliance</h2>
<table>
  <tr><th>Regulation</th><th>XAI Requirement</th><th>Skolyn Implementation</th></tr>
  <tr><td>EU AI Act (High-Risk)</td><td>Transparency, interpretability</td><td>Full XAI suite for all predictions</td></tr>
  <tr><td>FDA 21 CFR Part 820</td><td>Design validation documentation</td><td>XAI validation reports per model</td></tr>
  <tr><td>EU MDR Annex I</td><td>Information to users on limitations</td><td>Confidence intervals + known limitations</td></tr>
  <tr><td>GDPR Art. 22</td><td>Right to explanation</td><td>Per-prediction rationale generation</td></tr>
</table>

<h2>7. Document Revision History</h2>
<table>
  <tr><th>Version</th><th>Date</th><th>Author</th><th>Changes</th></tr>
  <tr><td>1.0</td><td>2024-06-15</td><td>AI Research</td><td>Initial release</td></tr>
  <tr><td>2.0</td><td>2025-06-01</td><td>AI Research</td><td>Added NLG, counterfactual</td></tr>
  <tr><td>3.0</td><td>2026-02-10</td><td>AI Research</td><td>Updated regulatory, calibration</td></tr>
</table>

<div class="signature-section">
  <div class="section-title">Authorization & Approval</div>
  <p style="font-size:13px;color:#5f6368;margin-bottom:24px;">This document has been reviewed and approved by the undersigned officers of Skolyn.</p>
  <div class="signature-grid">
    <div class="signature-block"><div class="role">Chief Technology Officer (CTO)</div><div class="name">Skolyn Technology Division</div><div class="signature-line"></div><div class="signature-label">Signature</div><div style="display:flex;justify-content:space-between;margin-bottom:16px"><div><span class="signature-label">Printed Name:</span> ___________________________</div></div><div><span class="signature-label">Date:</span> ______ / ______ / ____________</div><div class="stamp-area">Official Stamp / Seal</div></div>
    <div class="signature-block"><div class="role">Chief Executive Officer (CEO)</div><div class="name">Skolyn Executive Office</div><div class="signature-line"></div><div class="signature-label">Signature</div><div style="display:flex;justify-content:space-between;margin-bottom:16px"><div><span class="signature-label">Printed Name:</span> ___________________________</div></div><div><span class="signature-label">Date:</span> ______ / ______ / ____________</div><div class="stamp-area">Official Stamp / Seal</div></div>
  </div>
</div>

<div class="doc-footer"><p><strong>Skolyn</strong> — Redefining Medical Imaging Through Explainable AI</p><p>Document ID: SKL-AI-001 | Version 3.0 | Classification: Confidential</p><p>&copy; 2026 Skolyn. All rights reserved.</p></div>

</body>
</html>
